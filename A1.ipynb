{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Introduction to the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L_{\\text{BCE}}(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y),\n",
    "$$\n",
    "where $y$ is the label and $\\hat y$ is a prediction, which comes from a model (e.g. an RNN) and is usually sigmoid-activated, i.e., we have\n",
    "$$\n",
    "\\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "The argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit $z$ (instead of the prediction $\\hat y$) and incorporate the sigmoid activation into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions $L_{\\text{BCE}}(\\hat y, y)$ and $\\sigma(z)$ into one function $L(z, y) = L_{\\text{BCE}}(\\sigma(z), y)$. \n",
    "\n",
    "*Hint: Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$ and argue why the right-hand side is numerically more stable. Finally, express $L(z,y)$ in terms of that form.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "1.) Show $\\log(1 + e^z) = \\log(1 + e^{-|z|}) + \\max(0,z)$\n",
    "\n",
    "if $x <= 0$ this is obvious. \n",
    "\n",
    "if $x > 0$\n",
    "$$ \\log(1 + e^{-|z|}) + \\max(0,z) = \\log(1 + e^{-z}) + z$$\n",
    "$$ = \\log(1 + \\frac{1}{e^z}) + z$$\n",
    "$$ = \\log(\\frac{e^z + 1}{e^z}) + z$$\n",
    "$$ = \\log(1 + e^z) - z + z = \\log(1 + e^z) $$\n",
    "\n",
    "2.) The right-hand side is numerically more stable because if $z$ gets very large, then $e^z$ will get to large for a float value. You get \"inf\"\n",
    "\n",
    "3.) $$ L(z,y) = -y\\log\\sigma(z) - (1-y)\\log(1-\\sigma(z))$$\n",
    "$$  \\log \\frac{1}{1 + e^{-z}} = -\\log(1 + e^{-z}) = -\\log(1 + e^{-|z|}) - \\max(0,-z) $$\n",
    "$$  \\log (1 - \\frac{1}{1 + e^{-z}}) = \\log \\frac{e^{-z}}{1 + e^{-z}} = -z - \\log(1 + e^{-z}) = -z - \\log(1 + e^{-|z|}) - \\max(0,-z) $$\n",
    "\n",
    "Therefore we get:\n",
    "$$ L(z,y) = -y(\\log \\frac{1}{1 + e^{-|z|}} - \\max(0,-z)) - (1-y)(-z + \\log \\frac{1}{1 + e^{-|z|}} - \\max(0,-z)) $$\n",
    "$$ = -(-z + \\log \\frac{1}{1 + e^{-|z|}} - \\max(0,-z))  - yz $$\n",
    "\n",
    "$$ = z + \\log(1 + e^{-|z|}) + \\max(0,-z) - yz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.00671534848914\n",
      "275.00671534848914\n",
      "275.00671534848914\n",
      "275.00671534848914\n",
      "-5.006715348489118\n",
      "-5.006715348489118\n",
      "----\n",
      "-0.006715348489118056\n",
      "-0.006715348489118256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = 55\n",
    "z = -5\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def l1(y,z):\n",
    "    y_h = sigmoid(z)\n",
    "    return -y*np.log(y_h) - (1-y)*np.log(1-y_h)\n",
    "\n",
    "def l2(y,z):\n",
    "    return -y*(np.log(sigmoid(np.abs(z))) - max(0,-z)) - (1-y)*(-z + np.log(sigmoid(np.abs(z))) - max(0,-z))\n",
    "\n",
    "def l3(y,z):\n",
    "    return -(-z + np.log(sigmoid(np.abs(z))) - max(0,-z))  - y*z\n",
    "\n",
    "def l4(y,z):\n",
    "    return  z + np.log(1 + np.exp(-np.abs(z))) + max(0,-z) -y*z\n",
    "\n",
    "print(l1(y,z))\n",
    "print(l2(y,z))\n",
    "print(l3(y,z))\n",
    "print(l4(y,z))\n",
    "\n",
    "print(np.log(sigmoid(z)))\n",
    "print(-np.log(1 + np.exp(-np.abs(z))) - max(0,-z))\n",
    "print('----')\n",
    "print(np.log(1 - sigmoid(z)))\n",
    "print(-z -np.log(1 + np.exp(-np.abs(z))) - max(0,-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Initializing the network\n",
    "Consider the fully recurrent network\n",
    "$$\n",
    "s(t) = W x(t) + R a(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$. \n",
    "\n",
    "*Compared to the lecture notes we choose $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$. Further, we introduced an auxiliary variable $z(t)$ and transposed the weight matrices.*\n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "model = Obj()\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "def init(model, D, I, K):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.init = init\n",
    "model.init(D, I, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The forward pass\n",
    "Implement the forward pass for the fully recurrent network for sequence classification (many-to-one mapping). To this end, write a function `forward` that takes a `model`, a sequence of input vectors `x`, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the fully recurrent network and evaluate the (numerically stabilized) binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the logit $z(T)$ into `model.a` and `model.z`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, x, y):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: The computational graph\n",
    "\n",
    "Visualize the computational graph of the fully recurrent network unfolded in time. The graph should show the functional dependencies of the nodes $x(t), a(t), z(t), L(z(t), y(t))$ for $t \\in \\{1, 2, 3\\}$. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
